# -*- coding: utf-8 -*-
"""WorkingHeartFailurePrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iGyQVwwcIr0N_qKjU3dGvM_nBV2JI2z-
"""

# importing basic libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("heart.csv")
df.head()

x = df.drop('HeartDisease', axis = 1)
b = df.drop(['Sex', 'ChestPainType', 'RestingECG','ExerciseAngina','ST_Slope', 'HeartDisease'], axis = 1)
y = df['HeartDisease']

a = pd.get_dummies(x, drop_first=True)
a.head()

from sklearn.preprocessing import StandardScaler

b = pd.DataFrame(StandardScaler().fit_transform(b), columns = b.columns)

b.head()

a = a.drop(a.columns[[0,1,2,3,4,5]], axis =1)

a.head()

frames = [b,a]
a = pd.concat(frames, axis =1)

a.head()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

x = a

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)

clf = LogisticRegression(max_iter=500)
clf.fit(x_train, y_train)
clf.score(x_test, y_test)



from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix

clf = SVC()
clf.fit(x_train,y_train)

y_predict = clf.predict(x_test)

confusion_matrix(y_test, y_predict)

plot_confusion_matrix(clf, x_test, y_test)

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import plot_confusion_matrix

accuracy_score(y_test, y_predict)

print(classification_report(y_test, y_predict))

from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier()
dtc.fit(x_train, y_train)

y_predict2 = dtc.predict(x_test)

from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_predict2)

plot_confusion_matrix(dtc, x_test, y_test)

accuracy_score(y_test, y_predict2)

print(classification_report(y_test, y_predict2))

dtc.feature_importances_

pd.DataFrame(index = x.columns, data = dtc.feature_importances_, columns = ['Feature Importance'])

from sklearn.tree import plot_tree

plt.figure(figsize= (17,15))
plot_tree(dtc, filled = True, feature_names= x.columns)

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(class_weight= "balanced")

rfc.fit(x_train, y_train)

y_predict3 = rfc.predict(x_test)

accuracy_score(y_test, y_predict3)
#86.96% accuracy

confusion_matrix(y_test, y_predict3)

plot_confusion_matrix(rfc, x_test, y_test)

print(classification_report(y_test, y_predict3))

from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV



n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
max_features = ['auto', 'sqrt']
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]
rg = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

print(rg)

rfcr = RandomizedSearchCV(estimator= rfc, param_distributions= rg, n_iter= 100, cv = 3, verbose= 2, random_state= 42, n_jobs= -1)

rfcr.fit(x_train, y_train)
# Took 7.5 minutes for me. Here are results which one will get after running the above code
#Fitting 3 folds for each of 100 candidates, totalling 300 fits
#RandomizedSearchCV(cv=3,
#                   estimator=RandomForestClassifier(class_weight='balanced'),
#                   n_iter=100, n_jobs=-1,
#                   param_distributions={'bootstrap': [True, False],
#                                        'max_depth': [10, 20, 30, 40, 50, 60,
#                                                      70, 80, 90, 100, 110,
#                                                      None],
#                                         'max_features': ['auto', 'sqrt'],
#                                         'min_samples_leaf': [1, 2, 4],
#                                         'min_samples_split': [2, 5, 10],
#                                         'n_estimators': [200, 400, 600, 800,
#                                                          1000, 1200, 1400, 1600,
#                                                          1800, 2000]},
#                    random_state=42, verbose=2)

rfcr.best_params_

param_grid = {
    'bootstrap': [False],
    'max_depth': [100, 110, 120, 130],
    'max_features': [2, 3],
    'min_samples_leaf': [1,2,3],
    'min_samples_split': [1,2,3],
    'n_estimators': [500,600,700,800]
}



gs = GridSearchCV(estimator = rfc, param_grid = param_grid, cv = 3, verbose = 2, n_jobs= -1)

gs.fit(x_train, y_train)

gs.best_params_

new_rfc = RandomForestClassifier(class_weight ="balanced",bootstrap = False, max_depth = 100,max_features = 2,min_samples_leaf = 1,min_samples_split = 2, n_estimators = 700)

new_rfc.fit(x_train, y_train)

y_predict4 = new_rfc.predict(x_test)

accuracy_score(y_test, y_predict4)
#87.83% accuracy
#0.87% improvement

confusion_matrix(y_test, y_predict4)

plot_confusion_matrix(new_rfc, x_test, y_test)

print(classification_report(y_test, y_predict4))



